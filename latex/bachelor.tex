\documentclass[hyperref, bachelorofscience]{cgvpub}
% other document types next to bachelorofscience:
% masterofscience
% diplominf
% diplomist
% beleg

%more options (to be appended in the square brackets):
% german....... german version 
% female........ to be used for female endings in german
% bibnum....... numerical reference style
% final............ intended for the final submission
% lof.............. genereate list of figures
% lot.............. generate list of tables
% noproblem.. do not show task
% notoc......... do not generate table of contents
% twoside...... two sided layout


\author{Lucas Waclawczyk}
\title{Development of a Natural VR User Interface Using Haptic Gloves}
\birthday{26. April 1998}
\placeofbirth{Bad Langensalza}
\matno{4686687}
\betreuer{Benjamin Russig}
\bibfiles{lib.bib}
\problem{
\textbf{Motivation:} Interaction within a VR environment -- be it with application controls or the environment itself -- is usually mapped to the manipulation of some controller in current consumer-grade VR systems and applications. This causes an additional level of abstraction from the virtual world that can be detrimental to immersion. However, an increasing number of alternative controllers, usually in the form of gloves, that more closely match the interaction modes of the user's own hands are becoming available. They enable more accurate tracking of hand poses than is possible with affordable optical tracking technology while also being capable of providing haptic feedback.
The goal of this bachelor's thesis is the design, implementation and evaluation of a natural VR user interface that the user can manipulate directly using their hands. For matching the user's hand movements, the limited finger tracking provided by the AvatarVR\newcite{nd} gloves should be combined with a suitable hand pose generation strategy as well as Lighthouse-based position and orientation tracking\newcite{lighthouse}.

\textbf{Individual Tasks:}
\vspace{.3cm}
\begin{itemize}
	\item Literature review on VR glove technology and applications
	\item Design of a hand data model that is suitable for use with both the AvatarVR API and the Leap SDK 2.0\newcite{leaphand}
	\item Design and implementation of a strategy for context-sensitive generation of hand poses from AvatarVR finger orientation data
	\item Design of a suitable test scenario for a direct manipulation-based natural user interface (buttons, sliders, levers, etc.)
	\item Prototypical implementation of a CGV Framework plugin, with the following features: 
	VR rendering of the designed test scenario
	\item Real-time coupling of AvatarVR finger tracking with Lighthouse-based tracking of the glove position and orientation, and generation of haptic feedback from the tracking results
	\item Stylized rendering of the user's hand(s) using suitable Framework facilities (e.g. \lstinline|box_renderer|)
	\newpage
	\item Qualitative evaluation:
	\begin{itemize}
		\item Tracking precision and quality of generated hand poses
		\item User performance when interacting with the interface
	\end{itemize}
	\item Quantitative evaluation of tracking performance (e.g. latency)
\end{itemize}

\textbf{Optional Goals:}
\vspace{.3cm}
\begin{itemize}
	\item Comparative Evaluation:
	\begin{itemize}
		\item Controller-based interaction, e.g. via some controller-tailored abstract interface, or pointing and grabbing of the natural interface
		\item Full hand tracking via Leap Motion sensor
	\end{itemize}
	\item Incorporation of real-world objects into the natural user interface (e.g. a real table on which the virtual controls are situated) and analysis of the effect on perception of the controls
	\item Design and implementation of additional natural interaction modes not based on direct manipulation, e.g. gesture recognition
	\item Rendering of the user's hand(s) as skinned mesh
\end{itemize}	
}
\copyrighterklaerung{\textbf{Voyager Bridge Mesh}\\
Acquired from \url{https://www.trekmeshes.ch/} on 20 June 2020.

This mesh remains the property of Chainsaw\_NL and Star trek Meshes.
It however may be used to create images and scenes for non-profit use only.

Any images produced with this mesh do NOT have to be credited to the
mesh author. But it would be nice.

Star Trek and Enterprise are copyright Paramount Pictures.}
\acknowledgments{I'd like to thank Mr. Russig and Prof. Gumhold for their flexibility, creativity and availability. It was a pleasure to work on this together, sharing ideas and solutions.

I'd further like to thank my study participants and the many friends who gave their feedback during development - for their support, honesty and patience.

All of you made it possible to boldly do what noone had done before.}
\abstracten{
	Data Gloves are a technology that can be useful in many areas, especially because of the option of combined sensing and haptic feedback. Several projects have utilized them for scientific purposes, and numerous products are commercially available. This thesis gives an insight into their principles and some examples of applications. A VR scenario using the \Gls{AVR} and Lighthouse tracking is developped which places the user on the bridge of the USS Voyager (StarTrek) where they can control a virtual panel with the \Gls{AVR} to move through space and shoot at targets with phasers. A user study is performed to evaluate the scenario and technology.
}
\abstractde{
	Data Gloves sind eine Technologie, die in vielen Gebieten n\"utzlich sein kann, vor allem wegen der M\"oglichkeit gleichzeitiger Messung und haptischen Feedbacks. Verschiedene Projekte haben sie zu wissenschaftlichen Zwecken eingesetzt und zahlreiche Produkte sind kommerziell verf\"ugbar. Diese Arbeit gibt einen Einblick in ihre Prinzipien und einige Beispiele von Anwendungen. Ein VR-Szenario wird entwickelt, welches den \Gls{AVR} und Lighthouse-Tracking verwendet. Es versetzt Nutzer auf die Br\"ucke der USS Voyager (StarTrek), wo sie ein virtuelles Panel mit dem \Gls{AVR} steuern k\"onnen, um sich durch das Weltall zu bewegen und Ziele mit Phasern zu beschie\ss en. Eine Nutzerstudie wird durchgef\"uhrt, um das Szenario und die Technologie zu evaluieren.
}

\usepackage{subcaption}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{wrapfig}

\usepackage{glossaries}
\newglossary[tlg]{gloss_terms}{tld}{tdn}{Glossary}
\newglossary[slg]{gloss_acr}{sot}{stn}{Acronyms}
\newglossary{gloss_code}{glg}{gls}{Code}
\makeglossaries
\loadglsentries[gloss_terms]{gloss_terms}
\loadglsentries[gloss_acr]{gloss_acr}
\loadglsentries[gloss_code]{gloss_code}

\newcommand{\newcite}[1]{$ ^{\text{\cite{#1}}} $}

\begin{document}
\glsaddall
	
\chapter{Introduction}
In their review \emph{Haptic display for virtual reality: progress and challenges}\newcite{wang19}, Wang et al. state that \gls{vr} was born in 1965 with Ivan Sutherlands proposal of a concept named "The Ultimate Display"\newcite{sutherland65}. In that report of the IFIP Congress, Sutherland names immersion, interaction and imagination as three important features of \acrshort{vr}. 

There has been a multitude of approaches to making interaction with virtual environments possible\newcite{li19}. Touch screens and stylus pens have become quite common in everyday life. Devices capable of capturing scene depth like Microsoft Kinect and Leap Motion make touchless interaction possible, e.g. during games. This thesis focuses especially on wearable devices called \emph{Data Gloves}.

\section{Principles of Data Gloves}
The general principles of Data Gloves can be divided into two main parts: sensing and actuation. The first part deals with the issue of capturing a user's hand poses and motions and relaying that information to a computer for further processing. In most cases, a fabric glove is equipped with an \acrfull{imu} placed on each rigid hand part or flex sensors placed on joints. The latter have the advantage of measuring absolute values which, however, are only scalar and need further interpretation involving anatomical knowledge, e.g. that a sensor value of $ .7 $ corresponds to joint flexion of $ 45^{\circ} $. An \acrfull{imu}, on the other hand, usually measures instantaneous rates of change, like accelerations, but can be designed to return 3D rotations and positions by accumulating these measurements\newcite{imu}.

The matter of actuation belongs to the broader field of haptic feedback. According to Wang et al.\newcite{wang19}, the visual and auditory feedback of \acrshort{vr} systems has become fairly realistic over time. However, haptic feedback is still rather poor even though it is "[$\dots$] indespensable for enhancing immersion, interaction and imagination of \acrshort{vr} systems. Interaction can be enhanced by haptic feedback as users can directly manipulate virtual objects, and obtain immediate haptic feedback. Immersion of the \acrshort{vr} system can be enhanced in terms of providing more realistic sensation to mimic the physical interaction process. Imagination of users can be inspired when haptics can provide more cues for user[s] to mentally construct an imagined virtual world beyond spatial and / or temporal limitations."

\begin{figure}[t!]
	\centering
	\includegraphics[width=.8\linewidth]{../pics/wang01}
	\caption[Schematic paradigm of haptic human computer interfaces]{Schematic paradigm of haptic human computer interfaces consisting of three components, i. e. human user, interface device, and virtual environment; from \cite{wang19}}
	\label{fig:hci}
\end{figure}

A schematic of the general paradigm of haptic human computer interfaces is shown in figure \ref{fig:hci}. It consists of three main components:
\vspace{.3cm}
\begin{itemize}
	\item the user, who perceives virtual objects through biological receptors, processes them in the somatosensory and motor cortex, and requests manipulation by mechanical movement,
	\item the interface device which causes perception via actuation and collects information about requested manipulation with its sensors,
	\item the virtual environment which evaluates the interface device's data, realizes the manipulation of virtual objects with possible constraints, and motivates actuation in the interface device via haptic feedback.
\end{itemize}

As the following examples will demonstrate, not all Data Gloves are designed with the capability of both sensing and actuation. Some are only used to evaluate the user's hand pose, and it is conceivable to deal only with the issue of actuation and utilize an external device for sensing, such as Leap Motion. However, actuation on its own does not seem useful for VR applications, as it needs to be variable based on the current hand pose for which tracking of the user's hand is required.

\section{Examples and Applications of Data Gloves} \label{sec:ex}
\subsection{Sign Language Recognition}
In 2018, Kakoty et al. developed a data glove and processing system for the recognition of sign language alphabets (Indian, American and numbers) and their translation into speech\newcite{kakoty18}. In their paper, several earlier projects with similar purposes are reviewed, most of which are vision based. The authors point out that glove based systems are more useful because they "[$\dots$] allow one with the freedom for locomotion during the recognition process and [are] independent of the environmental lighting conditions."

\begin{figure}[b!]
	\centering
	\begin{subfigure}{.347\linewidth}
		\includegraphics[width=\linewidth]{../pics/kakoty_glove}
		\caption{Data glove developed by Kakoty et al.}
		\label{fig:kakoty_glove}
	\end{subfigure}
	\hspace{1cm}
	\begin{subfigure}{.4\linewidth}
		\includegraphics[width=\linewidth]{../pics/kakoty_principle}
		\caption{Proposed methodology for sign language recognition}
		\label{fig:kakoty_principle}
	\end{subfigure}
	\caption[A system for sign language recognition]{A system for sign language recognition; from \cite{kakoty18}}
	\label{fig:kakoty}
\end{figure}

The data glove used for this project is shown in figure \ref{fig:kakoty_glove}. It was constructed from a leather glove by equipping it with ten angle sensors, a microcontroller, and a transceiver module. The original paper describes this design in detail.

A preprocessing unit as in figure \ref{fig:kakoty_principle} handles the conversion from sensor output voltage to denoised, constrained standardized feature vectors. These are then processed by a radial base function kernel support vector machine and finally, the recognized sign language is translated into speech using label matching with a pre-recorded speech database. The whole process is implemented to run in real-time with the sensor data acquisition.

With an average recognition rate of 96.7\%, this approach achieves similar results as previous projects while recognizing more alphabets. Kakoty et al. further conclude that their approach "[$\dots$] overcomes the limitations of vision based systems. [$\dots$] An extension of the presented work for sentence-based sign language recognition will lead to an enterprising device; which is part of on-going research."

\subsection{Daily Activity Recognition}
Alzheimer's disease is a common cause for the loss of cognitive abilities amongst the elderly. Well known symptoms include language problems, loss of memory and degrading functioning of the hand. To better understand the latter, Maitre et al. investigated a method to recognize daily activities using a data glove developed specifically for that project\newcite{maitre19}.

\begin{figure}[b!]
	\begin{subfigure}{.57\linewidth}
		\includegraphics[width=\linewidth]{../pics/maitre_sensors}
		\caption{Arrangement of sensors on the hand}
		\label{fig:maitre_sensors}
	\end{subfigure}
	\hfill
	\begin{subfigure}{.3\linewidth}
		\includegraphics[width=\linewidth]{../pics/maitre_glove}
		\caption{Prototype device}
		\label{fig:maitre_glove}
	\end{subfigure}

	\vspace{.5cm}
	\begin{subfigure}{\linewidth}
		\includegraphics[width=\linewidth]{../pics/maitre_results}
		\caption{Evaluation of the recognition results}
		\label{fig:maitre_results}
	\end{subfigure}
	\caption[A data glove for recognition of basic daily activities]{A data glove for recognition of basic daily activities; from \cite{maitre19}}
	\label{fig:maitre}
\end{figure}

Several reasons are named as motivation to develop the device from scratch:
\vspace{.3cm}
\begin{itemize}
	\item Prices for commercial Data Gloves are situated between ca. 600 USD and several thousand USD. The authors propose a design that can be recreated for only about 220 USD.
	\item Commercial gloves are generally only compatible with Windows, but the SDK is not suitable for many other sensors.
	\item The new design can be specialized for recognizing the required types of grasp.
\end{itemize}

Discerning a daily activity is based on recognizing the involved item, e.g. a plate for the task \emph{Hold a plate}. For this, the presence of an object needs to be detected and information about its shape is required. Figure \ref{fig:maitre_sensors} shows the resulting sensor locations, where red and yellow mark flex sensors and blue marks force sensors. A prototype device can be seen in figure \ref{fig:maitre_glove}. 

A group of nine participants was asked to perform the daily activities listed in the evaluation tables in \ref{fig:maitre_results}. The acquired data sets were used to train (70\%) and test (30\%) a k Nearest Neighbours (k-NN) model whose precision (P), recall (R) and f-score (F) can be found in the left table, and a random forest (RF) to which the right table refers.

Maitre et al. conclude that the performance of their approach is "[$\dots$] almost perfect [$\dots$]. This result is surprising and encouraging [$\dots$]." Another experiment shows that both models perform significantly worse on data collected from a person that was not recorded during training. The authors therefore suggest to train the models with data from a patient that is supposed to use the system. The device can then even be worn in the comfort of the patient's home to monitor their hand activity.

\subsection{Refined Myoelectric Control in Below-Elbow Amputees}
\begin{figure}
	\centering
	\begin{subfigure}{.35\linewidth}
		\includegraphics[width=\linewidth]{../pics/amputees_poses}
		\caption{Hand poses performed during training data acquisition}
		\label{fig:amputees:poses}
	\end{subfigure}
	\hfill
	\begin{subfigure}{.3\linewidth}
		\includegraphics[width=\linewidth]{../pics/amputees_a}
		\caption{Recognition results of phantom hand movements for a subject with traumatic amputation (Subject A)}
		\label{fig:amputees:a}
	\end{subfigure}
	\hfill
	\begin{subfigure}{.3\linewidth}
		\includegraphics[width=\linewidth]{../pics/amputees_f}
		\caption{Recognition results of phantom hand movements for a subject with congenital failure of formation (Subject F)}
		\label{fig:amputees:f}
	\end{subfigure}
	\caption[Hand poses used in \cite{sebelius05} and recognition results]{Hand poses used in \cite{sebelius05} and recognition results; in (b) and (c), grey bars depict average performance, black depicts best-session performance and white the reference glove performance; from \cite{sebelius05}}
	\label{fig:amputees}
\end{figure}

A group of PhD students in Sweden trained an \acrfull{ann} to recognize myoelectric activity of below-elbow amputees as certain hand poses\newcite{sebelius05}. To generate training data, five subjects with a traumatic amputation and one subject with a congenital failure of formation were asked to perform the movements shown in figure \ref{fig:amputees:poses} with their healthy hand and their phantom hand simultaneously.

The healthy hand was therewhile tracked with a data glove to match the performed movement to one of M1 - M10. The arm stump on the side of the amputation was covered with relatively many electrodes to record detailed information about the remaining myoelectric activity. The (pre-processed) data was then used to train a tree-structured \acrshort{ann}.

As shown in figure \ref{fig:amputees} and more diagrams from the original paper, certain poses were recognized especially well or badly for certain subjects. An interesting process to see was how the performance of the subjects became more accurate over the course of the experiment. As this revealed itself within minutes of a session, the authors theorized, it might be due to unmasking of existing brain structures instead of the formation of new ones. 

The subject with congenital failure was of special interest because it showed that some movements could be accurately induced in the phantom hand even though they had never been executed before, possibly indicating the awakening of sleeping motor cortical areas in the brain that had been present since birth. Among the subjects with traumatic amputations, the accuracy and training effect did not seem related to the time since the amputation.

An important aspect for the training effect might be the simultaneous execution of movements with both the healthy and the phantom hand, as suggested in the original paper. The authors moreover suppose that a subject's performance might increase with visual feedback of a virtual phantom hand that was not given during these experiments. This thesis might be useful for that purpose, as it defines a virtual hand model for the use with Data Gloves, based on a well established hand model, in section \ref{sec:hand_model}. Work in this direction was continued by the authors of the original paper\newcite{sebelius06}.

\subsection{A Lightweight Force-Feedback Glove}
\begin{figure}
	\centering
	\includegraphics{../pics/force_feedback_proto}
	\caption{Physical prototype of a lightweight force-feedback glove with large workspace; from 
		\cite{zheng18}}
	\label{fig:force_feedback_proto}
\end{figure}

In terms of haptic feedback, a recent project is the \emph{Design of a Lightweight Force-Feedback Glove with a Large Workspace}\newcite{zheng18}. From their review of existing technology, the authors of the respective paper conclude the following "[$\dots$] requirements of ideal force feedback for VR interaction":

\newpage
\begin{itemize}
	\item The device weight should not exceed $ 100g $. This value is based on the success of desktop force-feedback devices, e.g. the PHANTOM Desktop\newcite{phantom}. The prototype shown in figure \ref{fig:force_feedback_proto} has a final mass of $ 245g $ which was achieved by a pneumatically actuated design with an outsourced construction for air compression and regulation. Force can be excerted onto the fingers by releasing compressed air into the pneumatic pistons (silver) which then pull back the links (black). Special materials were chosen for weight reduction: the links are made from $ 2mm $ thick carbon. Lightweight plastic was used for the base parts and finger caps (white).
	\item Typical grasping motions should be possible to simulate realistically. The device should therefore have a large motion and force range. The form of the links is specifically designed to ensure the necessary freedom of motion and makes it possible to mount the construction on the dorsal side of the hand. The authors emphasize that the key to obtaining optimal normal force at the finger cap while ensuring free movement of the finger joints is the curved profile of the sliding slot connecting the base components to the links. Its design and the respective calculations are described in detail in the original paper.
	\item "[$\dots$] the gloves should be easy to put on and remove, and should be adaptable for users with different hand sizes." This could, however, not be achieved and is suggested for future work.
\end{itemize}

A commercial data glove was used to track the user's hand. The pose was then evaluated by a PC that coordinated the release of air from the compression system into the pistons via a microcontroller. 

Experiments showed that the prototype developed by Zheng et al. can excert forces between $ 0.1N $ during free motion of the finger joints and $ 4N $ for constrained space simulation. It does indeed have a large working space in which a minimal virtual sphere with a diameter of $ 30mm $ can be simulated.

However, the normal force at the finger cap could not be controlled accurately. According to the authors, a possible solution might be the use of a force sensor at the finger tip to design a closed-loop control system. Moreover, the force-feedback showed significant delay due to the time of airflow between compression system and pistons.

For future work, the original paper suggests the development of new control hardware and software. They see the possibility of integrating the glove with a \acrfull{hmd} for novel applications such as e-shopping and plan to add components for haptic feedback on the hand palm as well.

\subsection{Commercial Products}
Numerous types of Data Gloves have been available on the market over the past years. Some examples capable of haptic feedback are shown in figure \ref{fig:commercial_gloves} and a summary of their technological properties is compiled in table \ref{tab:commercial_gloves}. Another example is the Avatar VR glove which will be used for the purposes of this thesis and explained in more detail in chapter \ref{ch:devel}.

\begin{figure}[b!]
	\includegraphics[width=\linewidth]{../pics/commercial_gloves}
	\caption{Examples of commercial Data Gloves as named in table \ref{tab:commercial_gloves} (fltr, top-row first); from \cite{wang19}}
	\label{fig:commercial_gloves}
\end{figure}

\begin{table}
	\footnotesize
	\centering
	\begin{tabularx}{\textwidth}{llXXXXXX}		
		\toprule[2pt] \midrule
		\textbf{Device name} & \textbf{Company} & \textbf{Motion tracking DoF} & \textbf{Force feedback} & \textbf{Tactile feedback} & \textbf{Actuation principle} & \textbf{Sensing principle} & \textbf{Typical features}\\
		\midrule
		CyberGrasp & Immersion & --- & One actuator per finger & --- & Electric motor & 22-sensor Cyber-Glove device & Feel the size and shape \\
		\midrule
		H-glove & Haption & Each finger posesses 3 DoF & Force feedback on 3 fingers & --- & Electric motor & --- & Can be attached to a Virtuose 6D\newcite{virtuose6d}\\
		\midrule
		Dexmo & DextaRobotics & Tracking 11 DoF for hand & Force feedback on 5 fingers & Three linear resonant actuators & Electric motor & Rotary sensors & Feel the shape, size and stiffness \\
		\midrule
		Haptx glove & Haptx & Tracking 6 DoF per digit & Light-weight exoskeleton applies up to $ 4lbs $ per finger & 130 stimuli points & Microfluidic array & Magnetic tracking & Feel shape, texture, motion in sub-mm precision\\
		\midrule
		Plexus & Plexus & 21 DoF per hand & --- & One tactile actuator per finger & --- & Using tracing adapters for other technology & Track with .01 deg precision\\
		\midrule
		VRgluv & VRgluv & 12 DoF for the fingers on each hand & Apply up to $ 5lbs $ of varying force per finger & --- & DC motors & 5 sensors per finger & Simulate stiffness, shape, and mechanical features\\
		\midrule \bottomrule[2pt]
	\end{tabularx}
	\caption{Existing commercial haptic feedback gloves; adapted, from \cite{wang19}}
	\label{tab:commercial_gloves}
\end{table}

\chapter{Development} \label{ch:devel}
This chapter names the technology used for this project, defines a skeletal hand model for the \Gls{AVR} haptic glove, and describes the development of a natural VR user interface based on these concepts. The complete code can be found at \url{github.com/lucaswzyk/vr_ctrl_panel}.

\section{Devices and Software}
\begin{figure}[b!]
	\begin{subfigure}{.49\linewidth}
		\includegraphics[width=\linewidth]{../pics/devices_uncoupled}
		\caption{Uncoupled devices, colors mark coupling points}
		\label{fig:devices:unc}
	\end{subfigure}
	\hfill
	\begin{subfigure}{.49\linewidth}
		\includegraphics[width=\linewidth]{../pics/devices_coupled}
		\caption{Coupled devices on user hand}
		\label{fig:devices:cou}
	\end{subfigure}
	\caption[Devices used to track the user's hand]{Devices used to track the user's hand: \Gls{VIVE} tracker, coupling, Avatar VR}
	\label{fig:devices}
\end{figure}

For the purpose of thesis, I used a pair of haptic gloves called Avatar VR which is a registered trademark of NeuroDigital Technologies, S. L.\newcite{nd}, referred to as \acrshort{nd} hereafter..

To use the Avatar VR on a computer, one needs to download, install and run the \acrshort{nd} Suite. This program provides a background service and GUI for managing and accessing the devices. A connection between device and computer can be established via Bluetooth after which all sensor data is graphically displayed in the GUI.

\acrshort{nd} also supplies a developer's API in the form of three files (\lstinline|NDAPI.h|, \lstinline|NDAPI_x64.lib| and \lstinline|NDAPI_x64.dll| or \lstinline|x86| respectively) which need to be included in the project in a suitable manner. An instance of the \lstinline|NDAPI| defined in the header file can be used to access the sensors and actuators of devices connected to \acrshort{nd} Suite.

Additionally, a \Gls{VIVE} tracker\newcite{lighthouse} was used to determine the user's hand position and orientation. The Avatar VR can be mechanically connected to the tracker using a coupling that needs to be acquired separately. The devices named so far and the way of coupling them is shown in figure \ref{fig:devices}.

The foundation for the graphics software I developed is implemented in the \gls{CGV} framework provided by the chair of Computer Graphics and Visualization at TU Dresden. A user can view the \gls{VCP} plugin on a computer screen with the \lstinline|cgv_viewer| or on a \Gls{VIVE} \acrfull{hmd} simply by connecting one to the computer. The framework automatically interacts with the SteamVR and Lighthouse\newcite{lighthouse} technology required to determine position and orientation of the \Gls{VIVE} trackers and \acrshort{hmd}.

\section{Skeletal Hand Model for the Avatar VR Haptic Glove} \label{sec:hand_model}
\subsection{Abstraction of the Human Hand Skeleton}
\begin{figure}[b!]
	\begin{subfigure}{.5\linewidth}
		\includegraphics[width=\linewidth]{../pics/leap_anat}
		\caption{General bone structure of the human hand}
		\label{fig:hand_model:anat}
	\end{subfigure}
	\hfill
	\begin{subfigure}{.47\linewidth}
		\includegraphics[width=\linewidth]{../pics/leap_example}
		\caption{Screenshot of an example hand representation in the Leap Motion API version 2.0, cropped}
		\label{fig:hand_model:leap}
	\end{subfigure}
	\caption{A model of the human hand; from \cite{leaphand}}
	\label{fig:hand_model}
\end{figure}

The human hand can be roughly divided into the parts shown in figure \ref{fig:hand_model:anat}, i.e. the \emph{\glspl{MC}}, \emph{\glspl{PPh}}, \emph{\glspl{IPh}}, and \emph{\glspl{DPh}}. A joint connecting a \gls{MC} to a \gls{PPh} is called \emph{\gls{MPJ}}, and followed by a \emph{\gls{PIJ}} and a \emph{\gls{DIJ}}.

Even though the carpals (grey in figure \ref{fig:hand_model:anat}) are physiologically quite relevant for hand movement, they stay relatively fixed compared to the other bone sets, and can thus be omitted in our simplified model. In anatomical nomenclature, the thumb is composed of \gls{PPh} and \gls{DPh} only and follows a \gls{MC}. However, this model assumes a missing \gls{MC} and an existing \gls{IPh} instead which will be modelled by a 0-length \gls{MC} for uniformity reasons.

The Leap Motion API version 2.0\newcite{leaphand} uses the abstraction described above and adds
\vspace{.3cm}
\begin{itemize}
	\item an "end" joint per finger (at the end of the \gls{DPh})
	\item a joint diagonally across the palm from the \gls{MPJ} of the index
	\item a joint in the middle of the palm
\end{itemize}

The most common representation includes cylinders for the bones and spheres for the joints. It is shown in figure \ref{fig:hand_model:leap}.

\subsection{Technology and Function of the Avatar VR}
\begin{figure}
	\centering
	\begin{subfigure}{\linewidth}
		\includegraphics[width=\linewidth]{../pics/av_imus}
		\caption{"Motion" tab of ND Suite with orientations in an (optimistic) example hand pose}
		\label{fig:av_tech:imus}
	\end{subfigure}
	
	\vspace{.2cm}
	\begin{subfigure}{.45\linewidth}
		\includegraphics[width=\linewidth]{../pics/av_contacts}
		\caption{Positions of contact sensors}
		\label{fig:av_tech:contacts}
	\end{subfigure}
	\hspace{.3cm}
	\begin{subfigure}{.45\linewidth}
		\includegraphics[width=\linewidth]{../pics/av_actu}
		\caption{Positions of actuators}
		\label{fig:av_tech:actuators}
	\end{subfigure}
	\caption{Sensors and actuators of the Avatar VR;\\screenshots of \acrshort{nd} Suite (b and c cropped)}
\end{figure}

The Avatar VR is equipped with three kinds of sensors: 
\vspace{.3cm}
\begin{itemize}
	\item An \emph{\acrfull{imu}} is located on the \gls{IPh} of each finger to measure its 3D rotation. The thumb is even equipped with two \acrshort{imu}s (one per phalanx) and the palm with an \acrshort{imu} that can measure both 3D acceleration and 3D rotation. The "Motion" tab of \acrshort{nd} Suite shown in figure \ref{fig:av_tech:imus} depicts the measured orientations as orthonormal bases. 
	\item At the locations marked in blue in figure \ref{fig:av_tech:contacts}, the fabric is made of an electrically conductive material. These parts are used as contact sensors (short: contacts) to determine which of the marked locations are joined.
	\item The thumb has a flex sensor, that will not be relevant for this thesis.
\end{itemize}

Furthermore, there are actuators at the locations marked in blue in figure \ref{fig:av_tech:actuators}. These can be used to generate vibrations of variable intensity as haptic feedback in a way that "[$\dots$] the brain perceives it as 'Real Touch' input", according to the manufacturer.

\subsection{Using Data From the Avatar VR to Visualize the User's Hand} \label{subsec:avr_to_hand}
\begin{wrapfigure}{R}{7cm}
	\includegraphics[width=\linewidth]{../pics/joint_positions}
	\caption{Struct for managing positions of hand joints, only most important parts included}
	\label{fig:joint_positions}
	\vspace{1cm}
\end{wrapfigure}

The same model used by the Leap Motion API version 2.0 will also be used here. Leap captures absolute joint positions and optimizes them for visualization. In contrast, my approach for the Avatar VR is to use a fixed resting state geometry of the user's hand (bone lengths and resting state joint positions). At the time of a \lstinline|draw()| command, the current geometry is constructed according to the data given by the glove and the \Gls{VIVE} tracker. This is done with the help of the \lstinline|joint_positions| struct, an excerpt of which is shown in figure \ref{fig:joint_positions}.

First, the palm joints' positions are rotated according to the orientation given by the \Gls{VIVE} tracker and saved in \lstinline|positions|. The construction of each finger begins at the end joint of the \gls{DPh} which is initialized in \lstinline|positions| as $ (0, 0, 0) $, translated along the negative $ z $-axis by the hard-coded length of the \gls{DPh} and rotated by the respective quaternion (see below). To make a recursive translation possible later on, the method \lstinline|rotate()| applies the quaternion to the whole finger.

The other phalanges are constructed in the same manner, before the finger is translated to its \gls{MPJ}. Finally, the whole hand is scaled to model view space and translated to the model view position of the \Gls{VIVE} tracker.

Because the Avatar VR only returns one quaternion per finger (except for the thumb), I had to think of a way to distribute the orientation along all three phalanges: First, the Euler angles are calculated from the quaternion\newcite{quat_to_euler}:
\begin{align*}
\alpha \quad&=\quad \mbox{arctan} \frac {2(q_0 q_1 + q_2 q_3)} {1 - 2(q_1^2 + q_2^2)} \\
\beta \quad&=\quad \mbox{arcsin} (2(q_0 q_2 - q_3 q_1)) \\
\gamma \quad&=\quad \mbox{arctan} \frac {2(q_0 q_3 + q_1 q_2)} {1 - 2(q_2^2 + q_3^2)}
\end{align*}
where $ \alpha $ is the roll angle, $ \beta $ the pitch angle and $ \gamma $ the yaw angle, and the quaternion can be written as $ (q_{0}, q_{1}, q_{2}, q_{3}) $. For implementation, one has to use the \lstinline|atan2| function, because \lstinline|arctan| only returns values between $ -\frac{\pi}{2} $ and $ \frac{\pi}{2} $.

The \lstinline|NDAPI| quaternions live in a space where the $ x $-axis points to the right, the $ y $-axis points upwards, and the $ z $-axis points into the picture. In terms of geometry, the coordinates are negatively oriented. One might know this as the "left hand rule". 

However, \gls{VCP} is oriented with the $ z $-axis pointing out of the picture (positively oriented, "right hand rule"). This results in opposite senses of rotation around the $ x $- and $ y $-axes in the two coordinate systems, so $ q_{1} $ and $ q_{2} $ from \lstinline|NDAPI| need to be negated before use with \lstinline|cgv|. Then roll corresponds to bending the finger (flexion / extension), pitch to a left-right-motion (abduction / adduction) and yaw to a twisting motion humans can hardly perform with their fingers (pronation / supination).

Since the interphalangeal joints act as revolute joints, only a roll component can be applied to them. As shown in the code below, the roll angle is distributed according to three floats (\lstinline|rot_split|). I could not find a reference for physiologically plausible values of these numbers, but after some experimenting, $ (.5, .5, .25) $ is an option that gives a good approximation of most actual hand poses. It corresponds to phalanx rotation during grabbing. Yaw and pitch are completely executed in the \gls{MPJ} which is a spherical joint. 
\vspace{.3cm}
\begin{lstlisting}
	vec3 x(1, 0, 0), y(0, 1, 0), z(0, 0, 1);
	recursive_rotations[finger][PROXIMAL] = palm_rotation
		* quat(z, yaw) * quat(y, pitch) * quat(x, rot_split.x() * roll);
	recursive_rotations[finger][INTERMED] = quat(x, rot_split.y() * roll);
	recursive_rotations[finger][DISTAL] = quat(x, min(1.4f, rot_split.z() * roll));
\end{lstlisting}

\begin{figure}[b!]
	\begin{subfigure}{.235\linewidth}
		\includegraphics[width=\linewidth]{../pics/hand_radial_flat}
	\end{subfigure}
	\vspace{.1cm}
	\hspace{.01cm}
	\begin{subfigure}{.235\linewidth}
		\includegraphics[width=\linewidth]{../pics/model_radial_flat}
	\end{subfigure}
	\hfill
	\begin{subfigure}{.235\linewidth}
		\includegraphics[width=\linewidth]{../pics/hand_dorsal_flat}
	\end{subfigure}
	\hspace{.01cm}
	\begin{subfigure}{.235\linewidth}
		\includegraphics[width=\linewidth]{../pics/model_dorsal_flat}
	\end{subfigure}
	\vspace{.1cm}
	\begin{subfigure}{.235\linewidth}
		\includegraphics[width=\linewidth]{../pics/hand_radial_nat}
	\end{subfigure}
	\hspace{.01cm}
	\begin{subfigure}{.235\linewidth}
		\includegraphics[width=\linewidth]{../pics/model_radial_nat}
	\end{subfigure}
	\hfill
	\begin{subfigure}{.235\linewidth}
		\includegraphics[width=\linewidth]{../pics/hand_dorsal_nat}
	\end{subfigure}
	\hspace{.01cm}
	\begin{subfigure}{.235\linewidth}
		\includegraphics[width=\linewidth]{../pics/model_dorsal_nat}
	\end{subfigure}
	\begin{subfigure}{.235\linewidth}
		\includegraphics[width=\linewidth]{../pics/hand_radial_ball}
	\end{subfigure}
	\hspace{.01cm}
	\begin{subfigure}{.235\linewidth}
		\includegraphics[width=\linewidth]{../pics/model_radial_ball}
	\end{subfigure}
	\hfill
	\begin{subfigure}{.235\linewidth}
		\includegraphics[width=\linewidth]{../pics/hand_dorsal_ball}
	\end{subfigure}
	\hspace{.01cm}
	\begin{subfigure}{.235\linewidth}
		\includegraphics[width=\linewidth]{../pics/model_dorsal_ball}
	\end{subfigure}
	\caption{Example pose correspondences of user and virtual hand; one pose per line}
	\label{fig:hand_correspondences}
\end{figure}
Figure \ref{fig:hand_correspondences} shows correspondences of user hand poses with virtual hand poses. A good approximation is reached, however, the virtual hand poses are not unique. For example, figure \ref{fig:natfake} shows two user hand poses that lead to approximately the same outcome as the one shown in the middle line of figure \ref{fig:hand_correspondences} because of the similar orientations of their \gls{IPh}. This issue cannot be fixed because the \Gls{AVR} does not provide information about the other phalanges.

The \Gls{AVR}'s actuators give the user haptic feedback when one of the joint spheres collides with a virtual object. The details are explained in \ref{sec:devel:structure:hands}.

\begin{figure}
	\centering
	\begin{subfigure}{.4\linewidth}
		\includegraphics[width=\linewidth]{../pics/hand_radial_natfake1}
	\end{subfigure}
	\hspace{1cm}
	\begin{subfigure}{.4\linewidth}
		\includegraphics[width=\linewidth]{../pics/hand_radial_natfake2}
	\end{subfigure}
	\caption{Radial view of two user hand poses corresponding to a similar handpose as the one in the middle line of figure \ref{fig:hand_correspondences}}
	\label{fig:natfake}
\end{figure}

\section{Project Idea and Structure}
\begin{figure}
	\includegraphics[width=\linewidth]{../pics/uml}
	\caption[Class diagram of \gls{VCP}]{Class diagram of \gls{VCP}, only most relevant elements shown}
	\label{fig:uml}
\end{figure}

To give the VR user interface some context rather than just moving boxes, I decided to design it as a conn panel on the USS Voyager (Star Trek). The term \emph{conn} is short for \emph{control and navigation}\newcite{memalpha}, meaning the panel is used to stear the ship. The classes used to implement this and their interaction are shown in figure \ref{fig:uml}.

\subsection{Main class}
The main class and entry point carries the same name as the plugin itself. It is derived from the \gls{CGV} classes \lstinline|base|, \lstinline|drawable|, \lstinline|event_handler| and \lstinline|provider|, and is used for scene management and interaction with the framework functionalities.

Its capability to handle \lstinline|vr_pose_events| is used to catch tracker poses and \acrshort{hmd} poses (position and orientation) which are forwarded to the \lstinline|hands| and \lstinline|head_up_display| (see below) respectively during the \lstinline|draw()| call. 

It distributes \lstinline|draw()| commands among all relevant members (\lstinline|hand|, \lstinline|conn_panel|, \lstinline|mesh|, and \lstinline|headup_display|). In the process, it passes on the necessary function arguments which ensures laziness: recalculations of geometry and containment checks are only done when necessary.

This class further manages the calibration routine (see section \ref{sec:cal}). It is the only class registered via object registration.

\subsection{Bridge}
The \lstinline|mesh| class renders the Voyager bridge. It uses the basic functionality of \lstinline|mesh_render_info| provided in \gls{CGV} to load and draw. A triangular mesh was provided by Chainsaw\_NL and StarTrek Meshes (see Copyright). The faces belonging to the viewscreen were deleted to make place for a view to the outside.

Just like the Voyager bridge, the mesh has an approximately circular layout with its center located at $ (0, 0, 0) $. Because its front and view screen are situated on the positive $ z $-axis, it needs to be rotated around the $ y $-axis by $ 180^{\circ} $ for use with \gls{VCP}. This is achieved with a model view matrix.

\subsection{Space, Targets and Phasers}
Around the bridge, space is simulated by a spherical shell filled with stars which is implemented in \lstinline|space|. The user never actually changes position in model space. Instead, the stars are moved in the opposite direction of the simulated motion. The speed ahead can be set via the following method: 
\vspace{.3cm}
\begin{lstlisting}
	static void set_speed_ahead(space* s, float val) 
	{ 
		s->speed_ahead = val * s->max_speed_ahead; 
	}
\end{lstlisting}

This needs to be static so it is possible to set it as callback method in objects outside the \lstinline|space| class. Analogous methods and variables exist for all three rotational speeds that are named as in aeronautics, i.e. pitch around the $ x $-axis, yaw around the $ y $-axis, and roll around the $ z $-axis. 

This class also manages targets whose visibility can be toggled with \lstinline|toggle_targets()| and that can be eliminated with phasers (laser canons) via the \lstinline|static_fire()| method. 

More details and the geometry behind this class are described in section \ref{sec:space}.

\begin{figure}
	\begin{subfigure}{.49\linewidth}
		\includegraphics[width=\linewidth]{../pics/panel}
		\caption{Panel configuration with explanatory annotations (added manually, not visible in \gls{VCP})}
		\label{fig:panel}
	\end{subfigure}
	\hfill
	\begin{subfigure}{.49\linewidth}
		\includegraphics[width=\linewidth]{../pics/firing}
		\caption{Screenshot of \lstinline|cgv_viewer| showing bridge with conn panel and view screen while firing on targets}
		\label{fig:firing}
	\end{subfigure}
	\caption{Setup of conn panel and bridge}
\end{figure}

\subsection{Control Panel}
The actual interface is built and drawn by \lstinline|conn_panel|. This passes the \lstinline|controlled_space| and a callback function on to each \lstinline|panel_node| during construction and the \lstinline|containment_info| coming from a \lstinline|hand| during \lstinline|check_containments()|. 

In its \lstinline|draw()| method, \lstinline|conn_panel| queries the current panel geometry with \lstinline|panel_tree->get_geometry_rec()| and renders the result with a \lstinline|box_renderer| provided by \lstinline|cgv|. Each \lstinline|panel_node| saves the last geometry of itself and its children as \lstinline|group_geometry|, that is only updated when a change occurs, e.g. a child button is pushed and changes color. The \lstinline|conn_panel| is positioned right on top of the mesh's conn.

Each \lstinline|panel_node| is represented as a box or combinations of boxes placed relative to their parent element. During an \lstinline|on_touch()| call, the non-trivial derived classes (\lstinline|button|, \lstinline|hold_button|, \lstinline|slider|, \lstinline|pos_neg_slider| and \lstinline|lever|) use their callback function to manipulate the \lstinline|controlled_space|. 

The \lstinline|conn_panel| configuration used in \gls{VCP} is shown in figure \ref{fig:panel}. The pitch and yaw slider behave like a control onboard a plane, e.g. sliding the pitch away from the user initiates downward rotation. Figure \ref{fig:firing} shows a screenshot of the panel in action, while a user is trying to hit a target.

\subsection{Hands} \label{sec:devel:structure:hands}
Interaction with the \Gls{AVR} is managed by the singleton \lstinline|nd_handler| which can be used to pass function calls through to an instance of \lstinline|NDAPI|. This is frequently done in \lstinline|nd_device|, a class for querying sensor data from the gloves and converting it for better compatibility with \lstinline|cgv| and \gls{VCP}, e.g. the sign flip explained at the end of section \ref{sec:hand_model}.

Finally, the representation of the user's hands is implemented in \lstinline|hand|. It keeps track of several points and joints the details of which are described in section \ref{sec:hand_model}. During a \lstinline|draw()| call, \lstinline|vr_ctrl_panel| passes a pointer to its \lstinline|conn_panel| to each \lstinline|hand| which then passes on information about the current hand geometry wrapped as \lstinline|containment_info| to the \lstinline|conn_panel| for containment check. Each \lstinline|panel_node| containing some hand part can then react with its \lstinline|on_touch()| method. A \lstinline|map<int, float>| describing the contained positions and suggested vibration strengths is returned to the hand, enabling it to react as well.

The vibration strengths are calculated as an interpolation between a minimum and a maximum value:
\vspace{.3cm}
\begin{lstlisting}
	float strength = min_strength + sqrt(1 - dist / rad) * (max_strength - min_strength);
	strength = max(0, strength);
\end{lstlisting}
where \lstinline|dist| is the distance between the joint and the panel and \lstinline|rad| is the radius of the sphere representing the joint. This yields a \lstinline|strength| of \lstinline|min_strength| when the sphere is not touching the panel and increases to \lstinline|max_strength| like a square root with the sphere moving into the panel. This kind of interpolation is motivated by the perception of physical objects which also increases quickly when first touching an object and then less quickly when increasing the pressure. 

\subsection{Headup Display} \label{sec:hd}
To convey written information to the user, a rectangular white region showing black text can be displayed in front of their eyes. This is implemented in \lstinline|headup_display| which uses \lstinline|label_manager| provided by \lstinline|cgv|. Its visibility can be controlled by setting an empty text (invisible) or a non-empty text (visible).

Whenever the display is invisible, its \lstinline|draw(context& ctx, vec3 p, mat3 ori)| call simply returns. Otherwise, its position and orientation are updated so it is rendered $ .6 $ units in front of and $ .1 $ units above the user's eye position:
\vspace{.3cm}
\begin{lstlisting}
	vec3 vs_hmd = ori * vec3(.0f, .1f, -.6f);
	position = pos + vs_hmd;
	orientation = ori;
\end{lstlisting}

\section{Calibration} \label{sec:cal}
\begin{figure}
	\centering
	\includegraphics[width=.95\linewidth]{../pics/calibration}
	\caption{State diagram of the calibration routine}
	\label{fig:cal}
\end{figure}

To make the use of \gls{VCP} in different rooms and setups possible, a calibration routine is required. It follows the state diagram shown in figure \ref{fig:cal}. Generally, joined contacts are interpreted in the following way:
\vspace{.3cm}
\begin{itemize}
	\item thumb + index: acknowledge
	\item thumb + middle: decline
	\item palm + index: choice 1
	\item palm + index: choice 2
\end{itemize}

The routine can be activated by joining index and thumb of one hand for three seconds. The hand requesting calibration is saved. The calibration routine does not react to the contacts of the other hand. This is done for reasons of implementation and because users found it natural to control the calibration with a single hand. The current calibration is saved so the user can return to it after aborting. 

While in the "requested" stage, the headup display shows a countdown until calibration. After that, none of the objects in the scene are rendered anymore. The user is asked whether the their position should be taken from the \acrshort{hmd} or set manually. In case of the latter, one can tap thumb and index above their head to set the position from the current tracker pose.

After that, the hands are rendered again and a countdown of $ 5s $ is displayed. The user is instructed to hold their hands straight with the fingers pointing forward. During this countdown, the hands and the model view matrix are constantly recalibrated:
\vspace{.3cm}
\begin{lstlisting}
	vec3 panel_origin = average of tracker positions;
	vec3 new_z = user_pos - panel_origin;
	new_z.y() = 0;
	new_z.normalize();
	float angle_y = angle between new_z and (0, 0, 1);
	
	// panel_origin becomes coordinate origin
	mat4 model_view_mat = translation(panel_origin)
	// -new_z becomes view direction
		* rotation((0, 1, 0), angle_y)
	// bridge center becomes coordinate origin
		* translation(panel_pos_on_bridge);
	mat4 world_to_model = inv(model_view_mat);
	
	vector<quat> ref_quats = nd.get_quats_from_sensors();
	for (size_t i = 0; i < num_imus; i++)
	{
		ref_quats[i] = ref_quats[i].inverse();
	}
\end{lstlisting}

This gives the following equalities:

\begin{lstlisting}
	new_z == model_view_mat * (0, 0, 1, 1)
	(0, 0, 1, 1) == world_to_model * new_z
	
	unit_quat == ref_quats[i] * nd.get_quats_from_sensors()[i]    // for all i
\end{lstlisting}

The tracker positions are now always transformed to model space by multiplication with \lstinline|world_to_model| and the quaternions from the sensors are multiplied with the \lstinline|ref_quats| before use. Therefore, the current calibration pose is assumed as the new "unit", meaning it looks like a trivial pose. 

Finally, the conn panel reappears close to the hands. One can continue to adjust the panel position by moving their hands until satisfied. A final acknowledgement by tapping index and thumb ends the calibration. It can then be exported to a file which is automatically loaded at plugin start.

Between two calibration stages, a short feedback pulse is generated by all actuators simultaneously to notify the user of the change. When the calibration is finished, a final pulse runs through the actuators successively. 

At any time, the calibration can also be aborted by joining thumb and middle finger which results in three short actuator pulses. One can then choose to return to the previous calibration or keep the partially altered one.

The whole process is communicated to the user on the headup display. It appears with the initial countdown and disappears when returning to the "not calibrating" stage.

\section{Space, Targets and Phasers} \label{sec:space}
\begin{figure}[b!]
	\centering
	\begin{subfigure}{.63\linewidth}
		\includegraphics[width=\linewidth]{../pics/stars_sphere_schema}
		\caption{Shell with stars around the user}
		\label{fig:space:shell}
	\end{subfigure}
	\hfill
	\begin{subfigure}{.34\linewidth}
		\centering
		\includegraphics[width=.8\linewidth]{../pics/phasers}
		\caption{Target phaser interaction}
		\label{fig:space:phaser}
	\end{subfigure}
	\caption{Schematics of the geometry behind the \lstinline|space| class, see text for explanations}
	\label{fig:space}
\end{figure}

A schematic of the spherical shell used to simulate space is shown in figure \ref{fig:space}. An outer radius determines the overall size of the sphere. A spherical region around the user that does not contain any stars is defined by an inner radius. This should include the complete bridge mesh to assure stars are only visible to the user on the view screen. Congruent with other parts of the plugin, the flight and view direction points along the negative $ z $-axis. 

\newpage
Stars are rendered as small spheres whose radii follow a normal distribution. The shell is initialized with a constant number of stars with uniformally distributed positions. It saves the forward and angular speeds as float values. During a \lstinline|draw()| call, the method \lstinline|update()| calculates new positions for all stars. It can be summarized as follows:

\begin{algorithmic}
		\ForAll{stars}
			\State Move star away from origin
			\If{star is outside of shell}
				\State Respawn star
			\Else
				\State Rotate star around user
			\EndIf
		\EndFor
\end{algorithmic}

To somewhat simplify the math behind this calculation, I used the point on the outer hull right in front of the user as origin for the shell space, instead of the center of the shell. The origin is marked with a cross in figure \ref{fig:space}. A model view matrix then translates the shell to model space. 

When moving dead ahead (angular speeds are zero), stars are moved along the yellow "forward motion" arrows in figure \ref{fig:space:shell}. The bigger the angle between their position vector and the positive $ z $-axis, the slower they move. More precisely, the length of a star's position vector is increased by the product of the cosine of the angle between the position vector and the positive $ z $-axis, and the general distance it has covered, calculated from \lstinline|speed_ahead| and the time since the last update.

When a star moves outside the outer hull of the shell, its position is re-initialized randomly somewhere on a sphere of the spawn radius around the origin. It cannot be simply set to the origin because it needs a direction. The spawn radius should further not be too small in order to avoid the impression of stars spawning very densly. Stars moving into the free sphere around the user are mirrored at the user. As this only happens when a forward motion is applied, the star then keeps moving away behind the user.

An additional rotation is realized by simply rotating the stars around the user. After some experimenting, I realized the spawn sphere needs to be rotated inversely around the user as well to achieve a realistic experience and because otherwise, the newly spawned stars converge to a line at slow forward motion and fast rotations.

Because they behave quite similar to stars, the targets are managed in \lstinline|space| as well. Their positions are updated as described for the stars. Additionally, their radii are continously adjusted as in the following pseudo code:
\vspace{.3cm}
\begin{lstlisting}
	// value between 0 (at user_pos) and 1 (at outer shell)
	float dist_frac = target_pos_rel_to_user.length() / r_out;
	// value between 0 (at outer shell) and 1 (at user_pos)
	dist_frac = 1 - dist_frac;
	// realistic size change
	dist_frac = sqrt(dist_frac);
	// scale to basic target radius
	target_radius_loc = dist_frac * target_radius;
\end{lstlisting}

This results in a radius of $ 0 $ at the outer shell and of \lstinline|target_radius| at the user position which is never reached due to mirroring at the inner shell. The targets are not active at plugin start, meaning they are neither updated nor rendered. They can be activated using \lstinline|toggle_targets()| and deactivated with the same method.

When the \lstinline|fire()| method is called, phasers are rendered as cones running from the left and right of the view screen to the left and and right of the origin (cross in figure \ref{fig:space}) where there radius becomes $ 0 $. This gives the impression of infinite rays. All targets are checked if they have been hit as shown in the schematic in figure \ref{fig:space:phaser} (viewed from above) and the following pseudo code:
\vspace{.3cm}
\begin{lstlisting}
	vec3 target_rel_to_left = target_pos - left_phaser_end;
	vec3 orth_proj = dot(target_rel_to_left, left_phaser_dir) * left_phaser_dir;
	float dist = (target_rel_to_left - orth_proj).length();
	bool hit = dist < target_radius_loc;
\end{lstlisting}

In figure \ref{fig:space:phaser}, the dotted line corresponds to \lstinline|target_pos|, the long arrow to \lstinline|target_rel_to_left| and the short arrow to the normalized vector \lstinline|left_phaser_dir|. 

If a target has been hit, it is mirrored at the user position, removing it from the view screen. 

\chapter{Evaluation}
\section{Participating Subjects and Preparation}
For the purpose of a user study, I chose two Star Trek Fans (A: female, 21 years; B: male, 48 years; called \emph{Trekkies}) and two people not familiar with the franchise (C: female, 49 years; D: female, 11 years; called \emph{Non-Trekkies}). All of them were instructed how to wear the \Gls{AVR} and the \acrshort{hmd}, and had approximately $ 15min $ to familiarize themselves with the environment, i.e. wearing the gloves and \acrshort{hmd}, controlling the conn panel, moving through space and eliminating targets. 

\section{Qualitative Evaluations and Adjustments}
The subjects were then asked for their opinion and suggestions regarding the design and behavior of the plugin. Several improvements were made in correspondence with the subjects, and then tested again. The following parts give a summary of these evaluations and adjustments.

\subsection{Usage of the \Gls{AVR}} \label{subsec:avr}
\begin{figure}
	\centering
	\begin{subfigure}{.3\linewidth}
		\includegraphics[width=\linewidth]{../pics/glove_cal1}
		\caption{Right after calibration}
		\label{fig:glove_cal:trivial}
	\end{subfigure}
	\hfill
	\begin{subfigure}{.3\linewidth}
		\includegraphics[width=\linewidth]{../pics/glove_cal2}
		\caption{After putting on the glove}
		\label{fig:glove_cal:puton}
	\end{subfigure}
	\hfill
	\begin{subfigure}{.3\linewidth}
		\includegraphics[width=\linewidth]{../pics/glove_cal3}
		\caption{After waiting an additional $ 60s $}
		\label{fig:glove_cal:minute}
	\end{subfigure}
	\caption{"Motion" tab of \acrshort{nd} Suite at different times after the \Gls{AVR}'s intrinsic calibration; middle and right picture taken after calibration as suggested by the manufacturer}
	\label{fig:glove_cal}
\end{figure}

The \Gls{AVR} has an inner and an outer layer between which the electrical components are situated. The inner layer is made from a stretch fabric, probably to adjust to different hand sizes. However, the fabric does not slide onto the hand well and the layers are only sparsely connected, allowing them to shift against each other. This made it difficult for the subjects to put on the glove for the first time. It took about $ 30s $, not counting the time needed to correctly place the actuators on the fingers. With some practice, the whole process can be completed in about $ 10s $. Putting on a second glove, locking the wrist band, and activating the gloves usually required outside help. 

When activating the glove, an internal calibration sets the current pose of each \acrshort{imu} to a trivial one as shown in figure \ref{fig:glove_cal:trivial}. The manufacturer\newcite{nd} actually intends for the gloves to be calibrated while laying on a flat surface. However, when putting the glove on after this and laying the respective hand on the same surface in a similar pose, the measured orientations exhibit a noticeable difference from the user's hand pose as shown in figure \ref{fig:glove_cal:puton}. 

Furthermore, after performing some movements with the glove, the measured orientations sometimes keep changing, even when resting the glove on a surface again. The stronger the accelerations during these movements are (e.g. during shaking), the stronger this drifting effect becomes. Figure \ref{fig:glove_cal:minute} shows the measured orientations after resting the hand for $ 60s $ which implies that even putting on the glove carefully involves sufficient motion for drifting to occur.

In my experience, putting on the gloves first and activating them afterwards yields a more stable result, so the subjects were asked to do that. Another reason for this is that the user can hold their fingers together and straight forward, which is closer to the trivial pose assumed during calibration than the fanned out pose of a glove on a flat surface. Then however, the gloves should not move during calibration time which most of the subjects found hard to achieve.

The recalibration described in section \ref{sec:cal} can compensate for the absolute difference between the user's hand pose and the measured orientations, but not for the drifting effect. As it is the only way to trigger an intrinsic calibration, the gloves needed to be switched off and back on frequently. 

Since the computer used for the study does not have Bluetooth capabilities, an additional USB Bluetooth connector was used to establish a connection with the gloves. However, this proved unstable for a single glove which disconnected in a matter of minutes or when moving further than $ 1m $ away from the connector, and it mostly did not work to connect both gloves at the same time. 

One possible explanation might be an insuffiency of the external connector, as the connection was stable for a longer time on a device with integrated Bluetooth capabilities, but it was lost there randomly as well. Also, the Bluetooth capable \Gls{VIVE} trackers placed directly on the glove might have an interfering influence.

To avoid interruptions, the gloves were connected to the computer via Micro USB for the user study. The cables had to be fixed on the users' arms with a sweatband to relieve the physical strain on the gloves' ports.

The subjects described the gloves as "restrictive" when touching physical objects which can be attributed especially to the actuators that are situated on the most sensitive spots of the skin. However, usage in VR was regarded as "unhindered". Subject A mentioned that a grabbing motion is slightly obstructed. The glove is light and, generally, the fabric moves easily. 

A single \acrshort{imu} per finger makes a perfect reconstruction of the user's hand pose impossible. Nonetheless, all subjects regarded the approximation described in part \ref{subsec:avr_to_hand} as "natural", "well usable", or similar.

\subsection{Haptip Feedback}
As an essential part of the whole project, the haptic feedback strength (see part \ref{sec:devel:structure:hands}) was adjusted in discussion with the subjects. After that, all subjects regarded the feedback when touching a virtual surface as "strangely realistic" or similar. 

I then applied different maximum vibration strengths for various panel components which none of the subjects found helpful for distinguishing between the components. Instead, subjects B and D regarded it as "unrealistic" and "confusing", so I went back to the original approach of a single maximum vibration strength throughout the whole panel. A good value for this strength seems to be $ .2 $ for use with the \Gls{AVR}.

When asked to reach through the conn panel, the subjects described a sense of intuitive hesitation that I could also see in their movements. Apparently, their brain had somewhat accepted the physical barrier formed by the virtual panel. 

\subsection{Timing}
During all sessions, the plugin ran smoothly. None of the subjects mentioned glitches or holds. 

To test the reaction time of the glove actuators, the conn panel was calibrated right onto a table edge. This was possible with the see-through function of the \acrshort{hmd} that is meant to keep users from running into physical objects, like a wall. All but subject D found the time difference between touching the table edge and haptic feedback from the gloves negligible, i.e. not sensible. During simulation, all subjects regarded the reaction time when touching a virtual object as negligible.

\subsection{Intuitive Grabbing and Lever Reaction}
\begin{figure}
	\centering
	\begin{subfigure}{.4\linewidth}
		\includegraphics[width=\linewidth]{../pics/grab1}
		\caption{Radial view}
		\label{fig:grab:radial}
	\end{subfigure}
	\hspace{1cm}
	\begin{subfigure}{.4\linewidth}
		\includegraphics[width=\linewidth]{../pics/grab2}
		\caption{Palmar view}
		\label{fig:grab:palmar}
	\end{subfigure}
	\caption{Intiutive grabbing performed by three of four subjects; index folds onto thumb instead of palmar below thumb}
	\label{fig:grab}
\end{figure}

Three of four subjects (except B) performed a grabbing motion intuitively as shown in figure \ref{fig:grab} with the index folding onto the thumb, above the contact on the palm. When designing the lever on the conn panel, I had anticipated a more palmar grabbing and therefore required both index and middle to be joined with the palm contact for the lever to become responsive. This is meant to keep the lever from changing its angle and value when incidentally touching it and rather to require an active grabbing to use it. However, it made the lever responsiveness unintuitive for these subjects.

To compensate, the condition of joined index and palm contacts was deleted. This does not significantly undermine the requirement of an intentional grabbing, as index extension combined with flexion of the other fingers is a difficult motion. The grabbing can thus still be sufficiently recognized by the middle and palm contacts. 

Subjects A, C and D described the adjusted lever control as "much simpler" and "more natural". Subject B stated that he did not feel a difference, supporting the statements above.

\subsection{Slider Reaction}
In the initial implementation, the sliders on the left side of the panel had been designed to react only to touches close to their current setting. This was meant to give the impression of a slider rather than buttons. However, the participants found it hard to match the current setting which resulted in problems with changing the slider value. This was adjusted so the user can tap anywhere on the slider to immediately change its value. A field was added in the middle of the slider that can be used to set its value to $ 0 $. 

\subsection{Targets and Phasers}
The target color was changed from red to green increasing the contrast to the orange phasers. The phaser color, position, and direction were adjusted to match the Trekkies impression of a phaser from the movies. The size of the targets represented by \lstinline|target_radius| above was adjusted until the subjects found it challenging, but possible to eliminate them. The "Toggle Targets" button was changed to stay green while the targets are active. The maximum rotation speed was adjusted until the subjects were comfortable with the resulting slider sensitivity.

\subsection{Calibration}
Instructions on the headup display during calibration lead to misunderstandings and miscalibrations. They were discussed with the subjects and adjusted until agreed upon.

Irritation was expressed because the subjects' hands ranged into the conn panel when it reappeared during calibration, triggering continuous vibration feedback. I added a vector \lstinline|hand_vs_panel_for_calibration| in the \lstinline|calibration| struct which moves the panel below the hands during calibration to avoid this.

\section{Quantitative Evaluation}
\begin{table}[b!]
	\begin{minipage}{.5\linewidth}
		\vspace{.47cm}
		\centering
		\begin{tabular}{|b{2cm}|c|c|c|c|}
			\hline
			Subject & A & B & C & D \\
			\hline
			Session 1 & 1:04 & 1:06 & 2:50 & 1:48 \\
			\hline
			Session 2 & 0:41 & 1:25 & 1:51 & 1:30 \\
			\hline
			Session 3 & 0:27 & 0:37 & 0:52 & 0:58 \\
			\hline
		\end{tabular}
		\captionof{table}{Time required to eliminate $ 5 $ targets}
		\label{tab:contest}
	\end{minipage}
	\hfill
	\begin{minipage}{.5\linewidth}
		\centering
		\includegraphics[width=\linewidth]{../pics/contest}
		\captionof{figure}{Time required to eliminate $ 5 $ targets}
		\label{fig:contest}
	\end{minipage}
\end{table}

Also quantitatively, the computation time turned out to be sufficiently small. The calculation took around $ 2-5ms $ per frame which is significantly lower than the $ 11.1ms $ ensuring $ 90fps $. When the plugin is started, it loads the bridge mesh within less than $ 3s $. The gloves transfer data with a framerate of ca. $ 80fps $ when connected via Bluetooth, and $ 200fps $ when using their Micro USB port. 

To evaluate the user performance, I measured the time subjects needed to eliminate $ 5 $ targets. Before each session, the gloves were intrinsically recalibrated by turning them off and back on. The targets were deactivated and the speed ahead set to maximum without rotation. The timer was started when pushing the "Toggle Targets" button. Each subject ran three such sessions.

Table \ref{tab:contest} and figure \ref{fig:contest} display the results of this experiment. All subjects increased their performance with each session, except subject B whose jumpy movements during session 2 caused his virtual hands to quickly diverge from the actual hand pose. This made it hard to use the controls with his fingers, and he had to finish the session using only the rigid parts of the palm to interact with the conn panel. The Trekkies performed noticeably better than the Non-Trekkies, who stated they had only little interest in and experience with VR in general. 

The overall improvement of the session times despite an earlier familiarization with the plugin might be due to the change of situation and the associated time pressure. In any case, the results prove an acceptable usability of the plugin.

\chapter{Conclusion}
For this thesis, I extensively reviewed literature on the principles of Data Gloves, available technology and some applications. There have been numerous, quite diverse approaches to human computer interaction involving only the user's hands. Many already are or might yet become useful, not only for VR, but medical and other projects as well. A few of them are summarized in section \ref{sec:ex}.

Furthermore, I familiarized myself with the \Gls{AVR} as well as tools to connect it to a computer, access its sensor data and control its actuators (\acrshort{nd} Suite and \lstinline|NDAPI|). A plugin coupling \Gls{AVR} finger tracking and Lighthouse position and orientation tracking was developed. It uses a hand model designed on the basis of human anatomy and the model used in Leap motion API version 2.0. 

The plugin scenario sends the user to the bridge of the USS Voyager where they can fly through a simulated space. Targets can be activated, deactivated, and eliminated with phasers. The geometry used for the simulation of space and for the phaser target interaction was explained thoroughly. To make the use of the plugin in different settings possible, a calibration routine was established. A user study led to numerous improvements of the plugin and revealed insights into its usability.

Some issues regarding the \Gls{AVR} can be found in part \ref{subsec:avr}. Especially the fact that it has only one \acrshort{imu} per finger made it challenging to recreate the user's hand pose realistically, which was possible by distributing the orientation of the \gls{IPh} onto the other phalanges in a physiologically plausible way. However, the device is most likely unfit for applications requiring more precision, e.g. the use with virtual musical instruments. In total, the glove was quite difficult to handle, already because of its instable connection to the computer and the drifting effect. 

Nonetheless, I could personally gain a lot from this project. It gave me an insight into scientific applications of Data Gloves. It  further deepened my understanding of programming in C++ and my knowledge about the \gls{CGV} framework which has even more features than I expected. The development of \gls{VCP} was an interesting challenge, the result of which works satisfyingly well and is even fun to use.

\printglossary[type=gloss_terms]
\printglossary[type=gloss_acr]
\end{document}